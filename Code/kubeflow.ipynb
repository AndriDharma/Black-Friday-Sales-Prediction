{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp import dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        Markdown)\n",
    "\n",
    "from kfp import compiler\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import pipeline_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please insert your service account path here\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"your Service account path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"dla-ml-specialization\"\n",
    "PIPELINE_ROOT = \"gs://dla-ml-specialization-dataset-2/pipelines/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\"\n",
    "    ],\n",
    ")\n",
    "def Load_from_BQ(\n",
    "    config: dict,\n",
    "    Train_data_BQ: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # 1. Load configuration file\n",
    "    config = config\n",
    "    # 2. Read data from BQ\n",
    "    client = bigquery.Client()\n",
    "    train_sql = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `dla-ml-specialization.demo_dataset_2.train`\n",
    "    \"\"\"\n",
    "    # Run a Standard SQL query with the project set explicitly\n",
    "    project_id = config['project_id']\n",
    "    train_data = client.query(train_sql, project=project_id).to_dataframe()\n",
    "\n",
    "    train_data.to_csv(f\"{Train_data_BQ.path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\",\n",
    "        \"scikit-learn==1.4.1.post1\"\n",
    "    ],\n",
    ")\n",
    "def Preprocessing(\n",
    "    config: dict,\n",
    "    Train_data_BQ: Input[Dataset],\n",
    "    x_trains: Output[Dataset],\n",
    "    x_tests: Output[Dataset],\n",
    "    y_trains: Output[Dataset],\n",
    "    y_tests: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # training data\n",
    "    df = pd.read_csv(f\"{Train_data_BQ.path}.csv\")\n",
    "    df.drop(['User_ID'], axis = 1, inplace = True)\n",
    "    df.drop(['Product_ID'], axis = 1, inplace = True)\n",
    "    df['Product_Category_2'] = df['Product_Category_2'].fillna(df['Product_Category_2'].mode()[0])\n",
    "    df['Product_Category_3'] = df['Product_Category_3'].fillna(df['Product_Category_3'].mode()[0])\n",
    "    df['Product_Category_1'] = df['Product_Category_1'].astype('object')\n",
    "    df['Product_Category_2'] = df['Product_Category_2'].astype('object')\n",
    "    df['Product_Category_3'] = df['Product_Category_3'].astype('object')\n",
    "    one_hot_encoded_data = pd.get_dummies(df, columns = ['Gender','City_Category','Stay_In_Current_City_Years','Age'], dtype=float)\n",
    "    train,test= train_test_split(one_hot_encoded_data,test_size = 0.2,random_state=42)\n",
    "\n",
    "    # Define your filter condition\n",
    "    filter_condition = train['Gender_F'] == 1  # Adjust 'value' according to your filter condition\n",
    "\n",
    "    # Apply the filter\n",
    "    filtered_df = train[filter_condition]\n",
    "\n",
    "    # random oversampling\n",
    "    oversample = filtered_df.sample(n=75000)\n",
    "    train = pd.concat([train, oversample], ignore_index=True)\n",
    "\n",
    "    x_test = test.drop(['Purchase'],axis=1)\n",
    "    y_test = test['Purchase']\n",
    "\n",
    "    x_train = train.drop(['Purchase'],axis=1)\n",
    "    y_train = train['Purchase']\n",
    "    \n",
    "    x_train.to_csv(f\"{x_trains.path}.csv\", index=False)\n",
    "    x_test.to_csv(f\"{x_tests.path}.csv\", index=False)\n",
    "    y_train.to_csv(f\"{y_trains.path}.csv\", index=False)\n",
    "    y_test.to_csv(f\"{y_tests.path}.csv\", index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\",\n",
    "        \"scikit-learn==1.4.1.post1\"\n",
    "    ],\n",
    ")\n",
    "def Training(\n",
    "    config: dict,\n",
    "    x_trains: Input[Dataset],\n",
    "    y_trains: Input[Dataset],\n",
    "    models: Output[Model]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    x_train = pd.read_csv(f\"{x_trains.path}.csv\")\n",
    "    y_train = pd.read_csv(f\"{y_trains.path}.csv\")\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 5)]\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [2,10]\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 3]\n",
    "\n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'min_samples_split': min_samples_split}\n",
    "\n",
    "\n",
    "    #defining model\n",
    "    rf_Model = RandomForestRegressor()\n",
    "\n",
    "    #tuning hyperparameter\n",
    "    rf_RandomGrid = RandomizedSearchCV(estimator = rf_Model, param_distributions = param_grid, cv = 2, verbose=2, n_jobs = 4)\n",
    "    rf_RandomGrid.fit(x_train, y_train)\n",
    "    params = rf_RandomGrid.best_estimator_.get_params()\n",
    "\n",
    "    #fitting model\n",
    "    RF = RandomForestRegressor(n_estimators=params['n_estimators'],\n",
    "                            max_depth=params['max_depth'],\n",
    "                            min_samples_leaf=params['min_samples_leaf'],\n",
    "                            min_samples_split=params['min_samples_split'],\n",
    "                            random_state=42)\n",
    "    RF.fit(x_train,y_train)\n",
    "\n",
    "    filename = f\"{models.path}.pkl\"\n",
    "    pickle.dump(RF, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\",\n",
    "        \"scikit-learn==1.4.1.post1\"\n",
    "    ],\n",
    ")\n",
    "def Evaluation(\n",
    "    config: dict,\n",
    "    x_tests: Input[Dataset],\n",
    "    y_tests: Input[Dataset],\n",
    "    models: Input[Model],\n",
    "    smetrics: Output[Metrics]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "\n",
    "    model_path = f\"{models.path}.pkl\"\n",
    "    model = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    x_test = pd.read_csv(f\"{x_tests.path}.csv\")\n",
    "    y_test = pd.read_csv(f\"{y_tests.path}.csv\")\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    smetrics.log_metric(\"mean_absolute_error\", mae)\n",
    "    smetrics.log_metric(\"r2_score\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#component deploy if endpoint already existed or haven't existed using if condition\n",
    "@component(base_image='python:3.10',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform==1.51.0\",\n",
    "        \"google-cloud-storage==2.14.0\"\n",
    "        ])\n",
    "def Deploy(\n",
    "    models: Input[Model],\n",
    "    config: dict):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    project = config['project_id']\n",
    "    region = config['region']\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    print(models)\n",
    "    print(models.uri)\n",
    "    import os\n",
    "    path,file = os.path.split(f\"{models.uri}.pkl\")\n",
    "\n",
    "    import datetime\n",
    "    \n",
    "    #moving model.pkl to a fixed gcs path\n",
    "    gcs_client = storage.Client()\n",
    "    gcs_bucket = gcs_client.get_bucket(\"dla-ml-specialization-dataset-2\")\n",
    "    #moving file from GCS source to sorted folder\n",
    "    name = str(models.uri)\n",
    "    new_path = name.replace(\"gs://dla-ml-specialization-dataset-2/\", \"\") \n",
    "    object_name = 'model.pkl'\n",
    "    destination_bucket = storage.Bucket(gcs_client, 'dla-ml-specialization-dataset-2')\n",
    "    source_blob = gcs_bucket.blob(f'{new_path}.pkl')\n",
    "    destination_name = f'model/{object_name}'\n",
    "    blob_copy = gcs_bucket.copy_blob(source_blob, destination_bucket, destination_name)\n",
    "\n",
    "    \n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"model_dataset_2\",\n",
    "        model_id = \"model_dataset_2\",\n",
    "        parent_model = \"1621700486132400128\", #existing model_id with the same model_id must exist\n",
    "        artifact_uri = \"gs://dla-ml-specialization-dataset-2/model/\",\n",
    "        serving_container_image_uri=\"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-2\",\n",
    "                            deployed_model_display_name = \"model_dataset_2\",\n",
    "                            min_replica_count=1,\n",
    "                            max_replica_count=1)\n",
    "    # if config['endpoint_id'] != None:\n",
    "    #     endpoint = aiplatform.Endpoint(\n",
    "    #     endpoint_name= config['endpoint_id'],\n",
    "    #     project=project,\n",
    "    #     location=region\n",
    "    #     )\n",
    "        \n",
    "    #     # datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    #     # serving image https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#xgboost\n",
    "    #     #upload model to mdoel registry\n",
    "    #     deployed_model = aiplatform.Model.upload(\n",
    "    #         display_name=\"model-dataset-2\",\n",
    "    #         model_id = \"model-dataset-2\",\n",
    "    #         parent_model = \"model-dataset-2\", #existing model_id with the same model_id must exist\n",
    "    #         artifact_uri = path,\n",
    "    #         serving_container_image_uri=\"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    #     )\n",
    "    #     #undeploy previous model version from endpoint\n",
    "    #     deployed_model_id = endpoint.gca_resource.deployed_models[0].id\n",
    "    #     endpoint.undeploy(deployed_model_id)\n",
    "\n",
    "    #     #deploy the new model version to the same endpoint as previous model \n",
    "    #     endpoint_model = deployed_model.deploy(\n",
    "    #         endpoint = endpoint,\n",
    "    #         deployed_model_display_name = \"model-dataset-2\",\n",
    "    #         machine_type=\"n1-standard-2\",\n",
    "    #         min_replica_count=1,\n",
    "    #         max_replica_count=1)\n",
    "    # else:\n",
    "    #     # datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    #     # serving image https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#xgboost\n",
    "    #     deployed_model = aiplatform.Model.upload(\n",
    "    #             display_name=\"model-dataset-2\",\n",
    "    #             model_id = \"model-dataset-2\",\n",
    "    #             artifact_uri = path,\n",
    "    #             serving_container_image_uri=\"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    #     )\n",
    "    #     endpoint = deployed_model.deploy(machine_type=\"n1-standard-2\",\n",
    "    #                             deployed_model_display_name = \"model-dataset-2\",\n",
    "    #                             min_replica_count=1,\n",
    "    #                             max_replica_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT + \"dataset-2-model\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"dataset-2-model\",\n",
    ")\n",
    "def pipeline(config: dict):\n",
    "    dataset_op = Load_from_BQ(config= config)\n",
    "    preproc_op = Preprocessing(config= config,\n",
    "                               Train_data_BQ= dataset_op.outputs['Train_data_BQ'])\n",
    "    training_op = Training(config= config,\n",
    "                           x_trains= preproc_op.outputs['x_trains'],\n",
    "                           y_trains= preproc_op.outputs['y_trains'])\n",
    "    eval_op = Evaluation(\n",
    "        config = config,\n",
    "        x_tests= preproc_op.outputs['x_tests'],\n",
    "        y_tests= preproc_op.outputs['y_tests'],\n",
    "        models= training_op.outputs['models']\n",
    "    )\n",
    "\n",
    "    deploy_op = Deploy(config= config,\n",
    "                       models= training_op.outputs['models'])\n",
    "\n",
    "    # with dsl.Condition(\n",
    "    #     eval_op.outputs[\"deploy\"] == \"true\",\n",
    "    #     name=\"deploy\",\n",
    "    # ):\n",
    "\n",
    "    #   deploy_op = Deploy(model = training_op.outputs[\"model\"], \n",
    "    #                      config= config)\n",
    "\n",
    "    # we need a solution for xgb models\n",
    "    # its here https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#aiplatform_deploy_model_custom_trained_model_sample-python\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='ml-specialization-dataset2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "config_dir =r\"config.yaml\"\n",
    "\n",
    "def load_config() -> dict:\n",
    "    # Try to load YAML file\n",
    "    try:\n",
    "        with open(config_dir, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except FileNotFoundError as fe:\n",
    "        raise RuntimeError(\"Parameters file not found in path.\")\n",
    "    \n",
    "    # Return params in dict format\n",
    "    return config\n",
    "\n",
    "def pickle_load(file_path: str):\n",
    "    # Load and return pickle file\n",
    "    return joblib.load(file_path)\n",
    "\n",
    "def pickle_dump(data, file_path: str) -> None:\n",
    "    # Dump data into file\n",
    "    joblib.dump(data, file_path)\n",
    "\n",
    "params = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"ml-specialization-dataset2-pipeline\",\n",
    "    template_path='ml-specialization-dataset2.json',\n",
    "    parameter_values={\"config\":config},\n",
    "    project = PROJECT_ID,\n",
    "    location= 'asia-southeast2',\n",
    "    enable_caching=False,\n",
    ")\n",
    "job.submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bni-dma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
