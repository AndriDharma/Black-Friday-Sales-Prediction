{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kfp import dsl\n",
    "from typing import NamedTuple\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        Markdown)\n",
    "\n",
    "from kfp import compiler\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import pipeline_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                         Version\n",
      "------------------------------- ---------------\n",
      "aiohttp                         3.9.3\n",
      "aiosignal                       1.3.1\n",
      "annotated-types                 0.6.0\n",
      "anyio                           4.3.0\n",
      "apache-beam                     2.54.0\n",
      "asttokens                       2.2.1\n",
      "attrs                           23.2.0\n",
      "backcall                        0.2.0\n",
      "cachetools                      5.3.2\n",
      "certifi                         2024.2.2\n",
      "cffi                            1.16.0\n",
      "charset-normalizer              3.3.2\n",
      "click                           8.1.7\n",
      "cloudpickle                     2.2.1\n",
      "colorama                        0.4.6\n",
      "comm                            0.1.3\n",
      "crcmod                          1.7\n",
      "cryptography                    42.0.5\n",
      "dataclasses-json                0.6.4\n",
      "db-dtypes                       1.2.0\n",
      "debugpy                         1.6.7\n",
      "decorator                       5.1.1\n",
      "Deprecated                      1.2.14\n",
      "dill                            0.3.1.1\n",
      "dnspython                       2.6.1\n",
      "docopt                          0.6.2\n",
      "docstring-parser                0.15\n",
      "exceptiongroup                  1.2.0\n",
      "executing                       1.2.0\n",
      "fastapi                         0.110.0\n",
      "fastavro                        1.9.4\n",
      "fasteners                       0.19\n",
      "frozenlist                      1.4.1\n",
      "fsspec                          2024.2.0\n",
      "gcsfs                           2024.2.0\n",
      "google-api-core                 2.17.1\n",
      "google-apitools                 0.5.31\n",
      "google-auth                     2.28.1\n",
      "google-auth-httplib2            0.1.1\n",
      "google-auth-oauthlib            1.2.0\n",
      "google-cloud-aiplatform         1.51.0\n",
      "google-cloud-bigquery           3.17.2\n",
      "google-cloud-bigquery-storage   2.24.0\n",
      "google-cloud-bigtable           2.23.0\n",
      "google-cloud-core               2.4.1\n",
      "google-cloud-datastore          2.19.0\n",
      "google-cloud-dlp                3.15.3\n",
      "google-cloud-language           2.13.3\n",
      "google-cloud-pubsub             2.20.1\n",
      "google-cloud-pubsublite         1.9.0\n",
      "google-cloud-recommendations-ai 0.10.10\n",
      "google-cloud-resource-manager   1.12.1\n",
      "google-cloud-spanner            3.43.0\n",
      "google-cloud-storage            2.14.0\n",
      "google-cloud-videointelligence  2.13.3\n",
      "google-cloud-vision             3.7.2\n",
      "google-crc32c                   1.5.0\n",
      "google-resumable-media          2.7.0\n",
      "googleapis-common-protos        1.62.0\n",
      "greenlet                        3.0.3\n",
      "grpc-google-iam-v1              0.13.0\n",
      "grpc-interceptor                0.15.4\n",
      "grpcio                          1.62.0\n",
      "grpcio-status                   1.62.0\n",
      "h11                             0.14.0\n",
      "hdfs                            2.7.3\n",
      "httplib2                        0.22.0\n",
      "httptools                       0.6.1\n",
      "idna                            3.6\n",
      "imbalanced-learn                0.12.0\n",
      "implicit                        0.7.2\n",
      "importlib_metadata              7.1.0\n",
      "ipykernel                       6.22.0\n",
      "ipython                         8.21.0\n",
      "jedi                            0.18.2\n",
      "joblib                          1.3.2\n",
      "Js2Py                           0.74\n",
      "jsonpatch                       1.33\n",
      "jsonpickle                      3.0.3\n",
      "jsonpointer                     2.4\n",
      "jsonschema                      4.21.1\n",
      "jsonschema-specifications       2023.12.1\n",
      "jupyter_client                  8.2.0\n",
      "jupyter_core                    5.3.0\n",
      "kfp                             2.7.0\n",
      "kfp-pipeline-spec               0.3.0\n",
      "kfp-server-api                  2.0.5\n",
      "kubernetes                      26.1.0\n",
      "langchain                       0.1.20\n",
      "langchain-community             0.0.38\n",
      "langchain-core                  0.1.52\n",
      "langchain-google-vertexai       0.1.2\n",
      "langchain-text-splitters        0.0.1\n",
      "langsmith                       0.1.38\n",
      "marshmallow                     3.21.1\n",
      "matplotlib-inline               0.1.6\n",
      "multidict                       6.0.5\n",
      "mypy-extensions                 1.0.0\n",
      "nest_asyncio                    1.6.0\n",
      "numpy                           1.24.4\n",
      "oauth2client                    4.1.3\n",
      "oauthlib                        3.2.2\n",
      "objsize                         0.7.0\n",
      "orjson                          3.9.15\n",
      "overrides                       7.7.0\n",
      "packaging                       23.2\n",
      "pandas                          2.2.0\n",
      "pandas-gbq                      0.21.0\n",
      "parso                           0.8.3\n",
      "pickleshare                     0.7.5\n",
      "pip                             24.0\n",
      "platformdirs                    3.5.0\n",
      "prompt-toolkit                  3.0.42\n",
      "proto-plus                      1.23.0\n",
      "protobuf                        4.25.3\n",
      "psutil                          5.9.5\n",
      "pure-eval                       0.2.2\n",
      "pyarrow                         11.0.0\n",
      "pyarrow-hotfix                  0.6\n",
      "pyasn1                          0.5.1\n",
      "pyasn1-modules                  0.3.0\n",
      "pycparser                       2.21\n",
      "pydantic                        2.6.2\n",
      "pydantic_core                   2.16.3\n",
      "pydata-google-auth              1.8.2\n",
      "pydot                           1.4.2\n",
      "Pygments                        2.15.1\n",
      "pyjsparser                      2.7.1\n",
      "PyJWT                           2.8.0\n",
      "pymongo                         4.6.2\n",
      "PyMuPDF                         1.24.0\n",
      "PyMuPDFb                        1.24.0\n",
      "pyparsing                       3.1.2\n",
      "pypdf                           4.1.0\n",
      "python-dateutil                 2.8.2\n",
      "python-dotenv                   1.0.1\n",
      "pytz                            2024.1\n",
      "pywin32                         306\n",
      "PyYAML                          6.0.1\n",
      "pyzmq                           25.0.2\n",
      "referencing                     0.33.0\n",
      "regex                           2023.12.25\n",
      "requests                        2.31.0\n",
      "requests-oauthlib               1.3.1\n",
      "requests-toolbelt               0.10.1\n",
      "rpds-py                         0.18.0\n",
      "rsa                             4.9\n",
      "scikit-learn                    1.4.1.post1\n",
      "scipy                           1.12.0\n",
      "setuptools                      70.0.0\n",
      "shapely                         2.0.3\n",
      "six                             1.16.0\n",
      "sniffio                         1.3.1\n",
      "SQLAlchemy                      2.0.29\n",
      "sqlparse                        0.4.4\n",
      "stack-data                      0.6.2\n",
      "starlette                       0.36.3\n",
      "tabulate                        0.9.0\n",
      "tenacity                        8.2.3\n",
      "threadpoolctl                   3.3.0\n",
      "tornado                         6.3.1\n",
      "tqdm                            4.66.2\n",
      "traitlets                       5.14.3\n",
      "types-protobuf                  4.24.0.20240311\n",
      "types-requests                  2.31.0.20240311\n",
      "typing_extensions               4.10.0\n",
      "typing-inspect                  0.9.0\n",
      "tzdata                          2024.1\n",
      "tzlocal                         5.2\n",
      "urllib3                         2.2.1\n",
      "uvicorn                         0.27.1\n",
      "watchfiles                      0.21.0\n",
      "wcwidth                         0.2.6\n",
      "websocket-client                1.7.0\n",
      "websockets                      12.0\n",
      "wheel                           0.43.0\n",
      "wrapt                           1.16.0\n",
      "xgboost                         2.0.3\n",
      "yarl                            1.9.4\n",
      "zipp                            3.17.0\n",
      "zstandard                       0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this for local but dont use it if using vertex ai workbench\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"your Service account path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"dla-ml-specialization\"\n",
    "PIPELINE_ROOT = \"gs://dla-ml-specialization-dataset-2/pipelines/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\"\n",
    "    ],\n",
    ")\n",
    "def Load_from_BQ(\n",
    "    config: dict,\n",
    "    Train_data_BQ: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # 1. Load configuration file\n",
    "    config = config\n",
    "    # 2. Read data from BQ\n",
    "    client = bigquery.Client()\n",
    "    train_sql = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `dla-ml-specialization.demo_dataset_2.train`\n",
    "    \"\"\"\n",
    "    # Run a Standard SQL query with the project set explicitly\n",
    "    project_id = config['project_id']\n",
    "    train_data = client.query(train_sql, project=project_id).to_dataframe()\n",
    "\n",
    "    train_data.to_csv(f\"{Train_data_BQ.path}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\",\n",
    "        \"scikit-learn==1.4.1.post1\"\n",
    "    ],\n",
    ")\n",
    "def Preprocessing(\n",
    "    config: dict,\n",
    "    Train_data_BQ: Input[Dataset],\n",
    "    x_trains: Output[Dataset],\n",
    "    x_tests: Output[Dataset],\n",
    "    y_trains: Output[Dataset],\n",
    "    y_tests: Output[Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # training data\n",
    "    df = pd.read_csv(f\"{Train_data_BQ.path}.csv\")\n",
    "    df.drop(['User_ID'], axis = 1, inplace = True)\n",
    "    df.drop(['Product_ID'], axis = 1, inplace = True)\n",
    "    df['Product_Category_2'] = df['Product_Category_2'].fillna(df['Product_Category_2'].mode()[0])\n",
    "    df['Product_Category_3'] = df['Product_Category_3'].fillna(df['Product_Category_3'].mode()[0])\n",
    "    df['Product_Category_1'] = df['Product_Category_1'].astype('object')\n",
    "    df['Product_Category_2'] = df['Product_Category_2'].astype('object')\n",
    "    df['Product_Category_3'] = df['Product_Category_3'].astype('object')\n",
    "    one_hot_encoded_data = pd.get_dummies(df, columns = ['Gender','City_Category','Stay_In_Current_City_Years','Age'], dtype=float)\n",
    "    train,test= train_test_split(one_hot_encoded_data,test_size = 0.2,random_state=42)\n",
    "\n",
    "    # Define your filter condition\n",
    "    filter_condition = train['Gender_F'] == 1  # Adjust 'value' according to your filter condition\n",
    "\n",
    "    # Apply the filter\n",
    "    filtered_df = train[filter_condition]\n",
    "\n",
    "    # random oversampling\n",
    "    oversample = filtered_df.sample(n=75000)\n",
    "    train = pd.concat([train, oversample], ignore_index=True)\n",
    "\n",
    "    X_train = train.drop(['Purchase'],axis=1)\n",
    "    y_train = train['Purchase']\n",
    "    X_test = test.drop(['Purchase'],axis=1)\n",
    "    y_test = test['Purchase']\n",
    "\n",
    "    x = train.drop(['Purchase'],axis=1)\n",
    "    y = train['Purchase']\n",
    "\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n",
    "    \n",
    "    x_train.to_csv(f\"{x_trains.path}.csv\", index=False)\n",
    "    x_test.to_csv(f\"{x_tests.path}.csv\", index=False)\n",
    "    y_train.to_csv(f\"{y_trains.path}.csv\", index=False)\n",
    "    y_test.to_csv(f\"{y_tests.path}.csv\", index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\",\n",
    "        \"scikit-learn==1.4.1.post1\"\n",
    "    ],\n",
    ")\n",
    "def Training(\n",
    "    config: dict,\n",
    "    x_trains: Input[Dataset],\n",
    "    y_trains: Input[Dataset],\n",
    "    models: Output[Model]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    x_train = pd.read_csv(f\"{x_trains.path}.csv\")\n",
    "    y_train = pd.read_csv(f\"{y_trains.path}.csv\")\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 5)]\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [2,10]\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 3]\n",
    "\n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'min_samples_split': min_samples_split}\n",
    "\n",
    "\n",
    "    #defining model\n",
    "    rf_Model = RandomForestRegressor()\n",
    "\n",
    "    #tuning hyperparameter\n",
    "    rf_RandomGrid = RandomizedSearchCV(estimator = rf_Model, param_distributions = param_grid, cv = 2, verbose=2, n_jobs = 4)\n",
    "    rf_RandomGrid.fit(x_train, y_train)\n",
    "    params = rf_RandomGrid.best_estimator_.get_params()\n",
    "\n",
    "    #fitting model\n",
    "    RF = RandomForestRegressor(n_estimators=params['n_estimators'],\n",
    "                            max_depth=params['max_depth'],\n",
    "                            min_samples_leaf=params['min_samples_leaf'],\n",
    "                            min_samples_split=params['min_samples_split'],\n",
    "                            random_state=42)\n",
    "    RF.fit(x_train,y_train)\n",
    "\n",
    "    filename = f\"{models.path}.pkl\"\n",
    "    pickle.dump(RF, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='python:3.10',\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"NumPy==1.24.4\",\n",
    "        \"SciPy==1.12.0\",\n",
    "        \"scikit-learn==1.4.1.post1\"\n",
    "    ],\n",
    ")\n",
    "def Evaluation(\n",
    "    config: dict,\n",
    "    x_tests: Input[Dataset],\n",
    "    y_tests: Input[Dataset],\n",
    "    models: Input[Model],\n",
    "    smetrics: Output[Metrics]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "\n",
    "    model_path = f\"{models.path}.pkl\"\n",
    "    model = pickle.load(open(model_path, 'rb'))\n",
    "\n",
    "    x_test = pd.read_csv(f\"{x_tests.path}.csv\")\n",
    "    y_test = pd.read_csv(f\"{y_tests.path}.csv\")\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test,y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    smetrics.log_metric(\"mean_absolute_error\", mae)\n",
    "    smetrics.log_metric(\"r2_score\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#component deploy if endpoint already existed or haven't existed using if condition\n",
    "@component(base_image='python:3.10',\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform==1.51.0\",\n",
    "        \"google-cloud-storage==2.14.0\"\n",
    "        ])\n",
    "def Deploy(\n",
    "    models: Input[Model],\n",
    "    config: dict):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    project = config['project_id']\n",
    "    region = config['region']\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    print(models)\n",
    "    print(models.uri)\n",
    "    import os\n",
    "    path,file = os.path.split(f\"{models.uri}.pkl\")\n",
    "\n",
    "    import datetime\n",
    "    \n",
    "    #moving model.pkl to a fixed gcs path\n",
    "    gcs_client = storage.Client()\n",
    "    gcs_bucket = gcs_client.get_bucket(\"dla-ml-specialization-dataset-2\")\n",
    "    #moving file from GCS source to sorted folder\n",
    "    name = str(models.uri)\n",
    "    new_path = name.replace(\"gs://dla-ml-specialization-dataset-2/\", \"\") \n",
    "    object_name = 'model.pkl'\n",
    "    destination_bucket = storage.Bucket(gcs_client, 'dla-ml-specialization-dataset-2')\n",
    "    source_blob = gcs_bucket.blob(f'{new_path}.pkl')\n",
    "    destination_name = f'model/{object_name}'\n",
    "    blob_copy = gcs_bucket.copy_blob(source_blob, destination_bucket, destination_name)\n",
    "\n",
    "    \n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"model_dataset_2\",\n",
    "        model_id = \"model_dataset_2\",\n",
    "        parent_model = \"1621700486132400128\", #existing model_id with the same model_id must exist\n",
    "        artifact_uri = \"gs://dla-ml-specialization-dataset-2/model/\",\n",
    "        serving_container_image_uri=\"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-2\",\n",
    "                            deployed_model_display_name = \"model_dataset_2\",\n",
    "                            min_replica_count=1,\n",
    "                            max_replica_count=1)\n",
    "    # if config['endpoint_id'] != None:\n",
    "    #     endpoint = aiplatform.Endpoint(\n",
    "    #     endpoint_name= config['endpoint_id'],\n",
    "    #     project=project,\n",
    "    #     location=region\n",
    "    #     )\n",
    "        \n",
    "    #     # datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    #     # serving image https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#xgboost\n",
    "    #     #upload model to mdoel registry\n",
    "    #     deployed_model = aiplatform.Model.upload(\n",
    "    #         display_name=\"model-dataset-2\",\n",
    "    #         model_id = \"model-dataset-2\",\n",
    "    #         parent_model = \"model-dataset-2\", #existing model_id with the same model_id must exist\n",
    "    #         artifact_uri = path,\n",
    "    #         serving_container_image_uri=\"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    #     )\n",
    "    #     #undeploy previous model version from endpoint\n",
    "    #     deployed_model_id = endpoint.gca_resource.deployed_models[0].id\n",
    "    #     endpoint.undeploy(deployed_model_id)\n",
    "\n",
    "    #     #deploy the new model version to the same endpoint as previous model \n",
    "    #     endpoint_model = deployed_model.deploy(\n",
    "    #         endpoint = endpoint,\n",
    "    #         deployed_model_display_name = \"model-dataset-2\",\n",
    "    #         machine_type=\"n1-standard-2\",\n",
    "    #         min_replica_count=1,\n",
    "    #         max_replica_count=1)\n",
    "    # else:\n",
    "    #     # datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    #     # serving image https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers#xgboost\n",
    "    #     deployed_model = aiplatform.Model.upload(\n",
    "    #             display_name=\"model-dataset-2\",\n",
    "    #             model_id = \"model-dataset-2\",\n",
    "    #             artifact_uri = path,\n",
    "    #             serving_container_image_uri=\"asia-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    #     )\n",
    "    #     endpoint = deployed_model.deploy(machine_type=\"n1-standard-2\",\n",
    "    #                             deployed_model_display_name = \"model-dataset-2\",\n",
    "    #                             min_replica_count=1,\n",
    "    #                             max_replica_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=PIPELINE_ROOT + \"dataset-2-model\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"dataset-2-model\",\n",
    ")\n",
    "def pipeline(config: dict):\n",
    "    dataset_op = Load_from_BQ(config= config)\n",
    "    preproc_op = Preprocessing(config= config,\n",
    "                               Train_data_BQ= dataset_op.outputs['Train_data_BQ'])\n",
    "    training_op = Training(config= config,\n",
    "                           x_trains= preproc_op.outputs['x_trains'],\n",
    "                           y_trains= preproc_op.outputs['y_trains'])\n",
    "    eval_op = Evaluation(\n",
    "        config = config,\n",
    "        x_tests= preproc_op.outputs['x_tests'],\n",
    "        y_tests= preproc_op.outputs['y_tests'],\n",
    "        models= training_op.outputs['models']\n",
    "    )\n",
    "\n",
    "    deploy_op = Deploy(config= config,\n",
    "                       models= training_op.outputs['models'])\n",
    "\n",
    "    # with dsl.Condition(\n",
    "    #     eval_op.outputs[\"deploy\"] == \"true\",\n",
    "    #     name=\"deploy\",\n",
    "    # ):\n",
    "\n",
    "    #   deploy_op = Deploy(model = training_op.outputs[\"model\"], \n",
    "    #                      config= config)\n",
    "\n",
    "    # we need a solution for xgb models\n",
    "    # its here https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#aiplatform_deploy_model_custom_trained_model_sample-python\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='ml-specialization-dataset2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project=PROJECT_ID)\n",
    "bucket = client.get_bucket('dla-ml-specialization-dataset-2')\n",
    "blob = bucket.blob('pipelines/ml-specialization-dataset2.json')\n",
    "blob.upload_from_filename('ml-specialization-dataset2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "config_dir =r\"C:\\Users\\ASUS\\Documents\\Job\\Datalabs\\2023-2024\\Specialization\\ML\\Code\\config.yaml\"\n",
    "\n",
    "def load_config() -> dict:\n",
    "    # Try to load YAML file\n",
    "    try:\n",
    "        with open(config_dir, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "    except FileNotFoundError as fe:\n",
    "        raise RuntimeError(\"Parameters file not found in path.\")\n",
    "    \n",
    "    # Return params in dict format\n",
    "    return config\n",
    "\n",
    "def pickle_load(file_path: str):\n",
    "    # Load and return pickle file\n",
    "    return joblib.load(file_path)\n",
    "\n",
    "def pickle_dump(data, file_path: str) -> None:\n",
    "    # Dump data into file\n",
    "    joblib.dump(data, file_path)\n",
    "\n",
    "params = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/8457519537/locations/asia-southeast2/pipelineJobs/dataset-2-model-20240722113053\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/8457519537/locations/asia-southeast2/pipelineJobs/dataset-2-model-20240722113053')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/asia-southeast2/pipelines/runs/dataset-2-model-20240722113053?project=8457519537\n"
     ]
    }
   ],
   "source": [
    "job = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"ml-specialization-dataset2-pipeline\",\n",
    "    template_path='ml-specialization-dataset2.json',\n",
    "    parameter_values={\"config\":config},\n",
    "    project = PROJECT_ID,\n",
    "    location= 'asia-southeast2',\n",
    "    enable_caching=False,\n",
    ")\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                         Version\n",
      "------------------------------- ---------------\n",
      "aiohttp                         3.9.3\n",
      "aiosignal                       1.3.1\n",
      "annotated-types                 0.6.0\n",
      "anyio                           4.3.0\n",
      "apache-beam                     2.54.0\n",
      "asttokens                       2.2.1\n",
      "attrs                           23.2.0\n",
      "backcall                        0.2.0\n",
      "cachetools                      5.3.2\n",
      "certifi                         2024.2.2\n",
      "cffi                            1.16.0\n",
      "charset-normalizer              3.3.2\n",
      "click                           8.1.7\n",
      "cloudpickle                     2.2.1\n",
      "colorama                        0.4.6\n",
      "comm                            0.1.3\n",
      "crcmod                          1.7\n",
      "cryptography                    42.0.5\n",
      "dataclasses-json                0.6.4\n",
      "db-dtypes                       1.2.0\n",
      "debugpy                         1.6.7\n",
      "decorator                       5.1.1\n",
      "Deprecated                      1.2.14\n",
      "dill                            0.3.1.1\n",
      "dnspython                       2.6.1\n",
      "docopt                          0.6.2\n",
      "docstring-parser                0.15\n",
      "exceptiongroup                  1.2.0\n",
      "executing                       1.2.0\n",
      "fastapi                         0.110.0\n",
      "fastavro                        1.9.4\n",
      "fasteners                       0.19\n",
      "frozenlist                      1.4.1\n",
      "fsspec                          2024.2.0\n",
      "gcsfs                           2024.2.0\n",
      "google-api-core                 2.17.1\n",
      "google-apitools                 0.5.31\n",
      "google-auth                     2.28.1\n",
      "google-auth-httplib2            0.1.1\n",
      "google-auth-oauthlib            1.2.0\n",
      "google-cloud-aiplatform         1.51.0\n",
      "google-cloud-bigquery           3.17.2\n",
      "google-cloud-bigquery-storage   2.24.0\n",
      "google-cloud-bigtable           2.23.0\n",
      "google-cloud-core               2.4.1\n",
      "google-cloud-datastore          2.19.0\n",
      "google-cloud-dlp                3.15.3\n",
      "google-cloud-language           2.13.3\n",
      "google-cloud-pubsub             2.20.1\n",
      "google-cloud-pubsublite         1.9.0\n",
      "google-cloud-recommendations-ai 0.10.10\n",
      "google-cloud-resource-manager   1.12.1\n",
      "google-cloud-spanner            3.43.0\n",
      "google-cloud-storage            2.14.0\n",
      "google-cloud-videointelligence  2.13.3\n",
      "google-cloud-vision             3.7.2\n",
      "google-crc32c                   1.5.0\n",
      "google-resumable-media          2.7.0\n",
      "googleapis-common-protos        1.62.0\n",
      "greenlet                        3.0.3\n",
      "grpc-google-iam-v1              0.13.0\n",
      "grpc-interceptor                0.15.4\n",
      "grpcio                          1.62.0\n",
      "grpcio-status                   1.62.0\n",
      "h11                             0.14.0\n",
      "hdfs                            2.7.3\n",
      "httplib2                        0.22.0\n",
      "httptools                       0.6.1\n",
      "idna                            3.6\n",
      "imbalanced-learn                0.12.0\n",
      "implicit                        0.7.2\n",
      "importlib_metadata              7.1.0\n",
      "ipykernel                       6.22.0\n",
      "ipython                         8.21.0\n",
      "jedi                            0.18.2\n",
      "joblib                          1.3.2\n",
      "Js2Py                           0.74\n",
      "jsonpatch                       1.33\n",
      "jsonpickle                      3.0.3\n",
      "jsonpointer                     2.4\n",
      "jsonschema                      4.21.1\n",
      "jsonschema-specifications       2023.12.1\n",
      "jupyter_client                  8.2.0\n",
      "jupyter_core                    5.3.0\n",
      "kfp                             2.7.0\n",
      "kfp-pipeline-spec               0.3.0\n",
      "kfp-server-api                  2.0.5\n",
      "kubernetes                      26.1.0\n",
      "langchain                       0.1.20\n",
      "langchain-community             0.0.38\n",
      "langchain-core                  0.1.52\n",
      "langchain-google-vertexai       0.1.2\n",
      "langchain-text-splitters        0.0.1\n",
      "langsmith                       0.1.38\n",
      "marshmallow                     3.21.1\n",
      "matplotlib-inline               0.1.6\n",
      "multidict                       6.0.5\n",
      "mypy-extensions                 1.0.0\n",
      "nest_asyncio                    1.6.0\n",
      "numpy                           1.24.4\n",
      "oauth2client                    4.1.3\n",
      "oauthlib                        3.2.2\n",
      "objsize                         0.7.0\n",
      "orjson                          3.9.15\n",
      "overrides                       7.7.0\n",
      "packaging                       23.2\n",
      "pandas                          2.2.0\n",
      "pandas-gbq                      0.21.0\n",
      "parso                           0.8.3\n",
      "pickleshare                     0.7.5\n",
      "pip                             24.0\n",
      "platformdirs                    3.5.0\n",
      "prompt-toolkit                  3.0.42\n",
      "proto-plus                      1.23.0\n",
      "protobuf                        4.25.3\n",
      "psutil                          5.9.5\n",
      "pure-eval                       0.2.2\n",
      "pyarrow                         11.0.0\n",
      "pyarrow-hotfix                  0.6\n",
      "pyasn1                          0.5.1\n",
      "pyasn1-modules                  0.3.0\n",
      "pycparser                       2.21\n",
      "pydantic                        2.6.2\n",
      "pydantic_core                   2.16.3\n",
      "pydata-google-auth              1.8.2\n",
      "pydot                           1.4.2\n",
      "Pygments                        2.15.1\n",
      "pyjsparser                      2.7.1\n",
      "PyJWT                           2.8.0\n",
      "pymongo                         4.6.2\n",
      "PyMuPDF                         1.24.0\n",
      "PyMuPDFb                        1.24.0\n",
      "pyparsing                       3.1.2\n",
      "pypdf                           4.1.0\n",
      "python-dateutil                 2.8.2\n",
      "python-dotenv                   1.0.1\n",
      "pytz                            2024.1\n",
      "pywin32                         306\n",
      "PyYAML                          6.0.1\n",
      "pyzmq                           25.0.2\n",
      "referencing                     0.33.0\n",
      "regex                           2023.12.25\n",
      "requests                        2.31.0\n",
      "requests-oauthlib               1.3.1\n",
      "requests-toolbelt               0.10.1\n",
      "rpds-py                         0.18.0\n",
      "rsa                             4.9\n",
      "scikit-learn                    1.4.1.post1\n",
      "scipy                           1.12.0\n",
      "setuptools                      70.0.0\n",
      "shapely                         2.0.3\n",
      "six                             1.16.0\n",
      "sniffio                         1.3.1\n",
      "SQLAlchemy                      2.0.29\n",
      "sqlparse                        0.4.4\n",
      "stack-data                      0.6.2\n",
      "starlette                       0.36.3\n",
      "tabulate                        0.9.0\n",
      "tenacity                        8.2.3\n",
      "threadpoolctl                   3.3.0\n",
      "tornado                         6.3.1\n",
      "tqdm                            4.66.2\n",
      "traitlets                       5.14.3\n",
      "types-protobuf                  4.24.0.20240311\n",
      "types-requests                  2.31.0.20240311\n",
      "typing_extensions               4.10.0\n",
      "typing-inspect                  0.9.0\n",
      "tzdata                          2024.1\n",
      "tzlocal                         5.2\n",
      "urllib3                         2.2.1\n",
      "uvicorn                         0.27.1\n",
      "watchfiles                      0.21.0\n",
      "wcwidth                         0.2.6\n",
      "websocket-client                1.7.0\n",
      "websockets                      12.0\n",
      "wheel                           0.43.0\n",
      "wrapt                           1.16.0\n",
      "xgboost                         2.0.3\n",
      "yarl                            1.9.4\n",
      "zipp                            3.17.0\n",
      "zstandard                       0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bni-dma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
